{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # !pip install --upgrade --force-reinstall  pytorch_lightning lightning_transformers \n",
    "# ! pip install git+https://github.com/PytorchLightning/lightning-transformers.git@master --upgrade\n",
    "# ! conda install -c conda-forge -y ipywidgets\n",
    "# ! jupyter nbextension enable --py widgetsnbextension\n",
    "# ! pip install lightning-flash\n",
    "# ! pip install Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "name_or_path = \"bert-large-uncased-whole-word-masking-finetuned-squad\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizerFast(name_or_path='bert-large-uncased-whole-word-masking-finetuned-squad', vocab_size=30522, model_max_len=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n",
      "The input has a total of 14 tokens.\n",
      "[101, 2129, 2003, 2023, 5304, 1029, 102, 2023, 5304, 2003, 6669, 29238, 1012, 102]\n"
     ]
    }
   ],
   "source": [
    "# Apply the tokenizer to the input text, treating them as a text-pair.\n",
    "tokenizer = AutoTokenizer.from_pretrained(name_or_path)\n",
    "print(tokenizer)\n",
    "\n",
    "question=\"How is this universe?\"\n",
    "context=\"This universe is perfectly imperfect.\"\n",
    "input_ids = tokenizer.encode(\n",
    "    question,\n",
    "    context, \n",
    ")\n",
    "\n",
    "print('The input has a total of {:} tokens.'.format(len(input_ids)))\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]           101\n",
      "how           2,129\n",
      "is            2,003\n",
      "this          2,023\n",
      "universe      5,304\n",
      "?             1,029\n",
      "\n",
      "[SEP]           102\n",
      "\n",
      "this          2,023\n",
      "universe      5,304\n",
      "is            2,003\n",
      "perfectly     6,669\n",
      "imperfect    29,238\n",
      ".             1,012\n",
      "\n",
      "[SEP]           102\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "# For each token and its id...\n",
    "for token, id in zip(tokens, input_ids):\n",
    "    \n",
    "    # If this is the [SEP] token, add some space around it to make it stand out.\n",
    "    if id == tokenizer.sep_token_id:\n",
    "        print('')\n",
    "    \n",
    "    # Print the token string and its ID in two columns.\n",
    "    print('{:<12} {:>6,}'.format(token, id))\n",
    "\n",
    "    if id == tokenizer.sep_token_id:\n",
    "        print('')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Search the input_ids for the first instance of the `[SEP]` token.\n",
    "sep_index = input_ids.index(tokenizer.sep_token_id)\n",
    "\n",
    "# The number of segment A tokens includes the [SEP] token istelf.\n",
    "num_seg_q = sep_index + 1\n",
    "\n",
    "# The remainder are segment B.\n",
    "num_seg_c = len(input_ids) - num_seg_q\n",
    "\n",
    "# Construct the list of 0s and 1s.\n",
    "segment_ids = [0]*num_seg_q + [1]*num_seg_c\n",
    "\n",
    "# There should be a segment_id for every input token.\n",
    "assert len(segment_ids) == len(input_ids)\n",
    "\n",
    "print(segment_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9697c1d2ed30c18493006365c6338d31ffdb61a9df8621b2c7ddc397089eb352"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('medqa')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
